{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "univariate feature selection (SelectKBest)\n",
        "automatically chooses classification vs regression, tunes"
      ],
      "metadata": {
        "id": "Ug5g82u-HdAd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWVt3x1zHacR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Detect task type (classification vs regression) from y_train\n",
        "def _is_classification(y: pd.Series) -> bool:\n",
        "    # object/category -> classification\n",
        "    if y.dtype.kind in (\"O\",) or str(y.dtype).startswith(\"category\"):\n",
        "        return True\n",
        "    # integer/bool with a reasonably small number of classes -> classification\n",
        "    if y.dtype.kind in (\"b\", \"i\", \"u\") and y.nunique(dropna=False) <= 50:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "IS_CLASSIFICATION = _is_classification(y_train)\n",
        "\n",
        "score_func = mutual_info_classif if IS_CLASSIFICATION else mutual_info_regression\n",
        "model = LogisticRegression(max_iter=1000, n_jobs=None) if IS_CLASSIFICATION else Ridge()\n",
        "\n",
        "# Use the scaled features created above\n",
        "Xtr, Xva, Xte = X_train_std, X_val_std, X_test_std\n",
        "\n",
        "# Candidate k values (donâ€™t exceed number of columns)\n",
        "n_feats = Xtr.shape[1]\n",
        "k_grid = [k for k in [5, 10, 15, 20, 30, 40, 60] if 1 <= k <= n_feats]\n",
        "if not k_grid:\n",
        "    k_grid = [min(10, max(1, n_feats))]\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"kbest\", SelectKBest(score_func=score_func, k=k_grid[0])),\n",
        "    (\"model\", model),\n",
        "])\n",
        "\n",
        "param_grid = {\"kbest__k\": k_grid}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=0,\n",
        ")\n",
        "grid.fit(Xtr, y_train)\n",
        "\n",
        "best_k = grid.best_params_[\"kbest__k\"]\n",
        "selector: SelectKBest = grid.best_estimator_.named_steps[\"kbest\"]\n",
        "\n",
        "# Transform datasets\n",
        "Xtr_sel_np = selector.transform(Xtr)\n",
        "Xva_sel_np = selector.transform(Xva)\n",
        "Xte_sel_np = selector.transform(Xte)\n",
        "\n",
        "# Recover kept column names\n",
        "kept_mask = selector.get_support()\n",
        "kept_cols = Xtr.columns[kept_mask].tolist()\n",
        "\n",
        "Xtr_sel = pd.DataFrame(Xtr_sel_np, index=Xtr.index, columns=kept_cols)\n",
        "Xva_sel = pd.DataFrame(Xva_sel_np, index=Xva.index, columns=kept_cols)\n",
        "Xte_sel = pd.DataFrame(Xte_sel_np, index=Xte.index, columns=kept_cols)\n",
        "\n",
        "# Small report of top features + scores\n",
        "scores = pd.Series(selector.scores_, index=Xtr.columns).dropna().sort_values(ascending=False)\n",
        "top10 = scores.head(10)\n",
        "\n",
        "print(f\"Task type: {'Classification' if IS_CLASSIFICATION else 'Regression'}\")\n",
        "print(f\"Best k selected via 5-fold CV: {best_k}\")\n",
        "print(f\"Best CV score (pipeline): {grid.best_score_:.4f}\")\n",
        "print(\"\\nTop 10 feature scores:\")\n",
        "print(top10)\n",
        "\n",
        "# Save selector + selected datasets\n",
        "OUT = Path(OUT) if not isinstance(OUT, Path) else OUT  # use your OUT dir from above\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "joblib.dump(selector, OUT / \"selector_kbest.pkl\")\n",
        "\n",
        "train_kbest = Xtr_sel.copy(); train_kbest[TARGET] = y_train.values\n",
        "val_kbest   = Xva_sel.copy();  val_kbest[TARGET]   = y_val.values\n",
        "test_kbest  = Xte_sel.copy();  test_kbest[TARGET]  = y_test.values\n",
        "\n",
        "train_kbest.to_csv(OUT / \"train_selected_kbest.csv\", index=False)\n",
        "val_kbest.to_csv(  OUT / \"val_selected_kbest.csv\",   index=False)\n",
        "test_kbest.to_csv( OUT / \"test_selected_kbest.csv\",  index=False)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(f\"- selector_kbest.pkl\")\n",
        "print(f\"- train_selected_kbest.csv\")\n",
        "print(f\"- val_selected_kbest.csv\")\n",
        "print(f\"- test_selected_kbest.csv\")\n",
        "print(f\"\\nKept {len(kept_cols)}/{n_feats} features.\")\n"
      ]
    }
  ]
}